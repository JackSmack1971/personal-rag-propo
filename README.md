# Personal RAG System ğŸ¤–ğŸ“š

<div align="center">

### **Next-Generation Personal Knowledge Assistant with Advanced MoE Retrieval**

*Transform your documents into an intelligent, citation-accurate research companion powered by state-of-the-art AI*

![Python](https://img.shields.io/badge/Python-3.11+-3776ab.svg?logo=python&logoColor=white)
![Gradio](https://img.shields.io/badge/Gradio-5.42.0+-ff7c00.svg?logo=gradio&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.8.0+-ee4c2c.svg?logo=pytorch&logoColor=white)
![Pinecone](https://img.shields.io/badge/Pinecone-7.0.0+-00d4ff.svg)
![Sentence Transformers](https://img.shields.io/badge/SentenceTransformers-5.1.0+-1f77b4.svg)

[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Build Status](https://img.shields.io/badge/Build-Passing-brightgreen.svg)]()
[![Version](https://img.shields.io/badge/Version-2025.1-blue.svg)]()
[![Security](https://img.shields.io/badge/Security-Enhanced-orange.svg)]()

[ğŸš€ Quick Start](#-installation-guide) â€¢ [ğŸ“– Documentation](#-usage-instructions) â€¢ [ğŸ— Architecture](#-visual-architecture-overview) â€¢ [ğŸ’¡ Features](#-key-features) â€¢ [ğŸ›  Development](#-development-guide)

</div>

---

## ğŸ¯ Quick Reference

The **Personal RAG System** is an advanced retrieval-augmented generation platform that transforms your personal documents into an intelligent, searchable knowledge base. Built with the latest 2025 technology stack, it combines cutting-edge AI models with production-ready infrastructure to deliver precise, citation-backed answers from your document collection.

### âœ¨ Key Features

- **ğŸ§  Advanced Document Intelligence**: Extracts atomic propositions from PDFs, TXT, and Markdown files using LLM-powered analysis
- **ğŸ” Hybrid Retrieval System**: Combines dense embeddings, sparse encoding, and Mixture of Experts (MoE) architecture for superior accuracy  
- **âš¡ Performance Optimized**: Multi-backend support (torch/onnx/openvino) with 4x performance improvements and intelligent caching
- **ğŸ¨ Modern Web Interface**: Production-ready Gradio 5.x UI with SSR, PWA support, and mobile optimization
- **ğŸ’° Cost Intelligence**: Real-time LLM cost monitoring with predictive analysis and automated alerts
- **ğŸ”’ Enterprise Security**: Authentication, rate limiting, SSL/HTTPS, and comprehensive security logging
- **ğŸ“Š Advanced Analytics**: Comprehensive evaluation metrics including Hit@k, nDCG@k, and A/B testing capabilities

### ğŸ›  Technology Stack

| Component | Version | Purpose | Performance Boost |
|-----------|---------|---------|------------------|
| **Frontend** | Gradio 5.42.0+ | Production web UI with SSR | 60-80% faster loading |
| **Vector DB** | Pinecone 7.0.0+ | Serverless vector storage | gRPC performance gains |
| **Embeddings** | Sentence-Transformers 5.1.0+ | Multi-backend encoding | 4x inference speedup |
| **ML Runtime** | PyTorch 2.8.0+ | Enhanced model inference | FP16 CPU optimizations |
| **Document Parser** | pypdf 6.0.0+ | Modern PDF processing | Enhanced security |
| **LLM Provider** | OpenRouter API | 100+ language models | Rate limiting & credits |

### ğŸª Live Demo

- **Main Application**: http://localhost:7860 (after setup)
- **Health Monitoring**: http://localhost:8000/health
- **Performance Metrics**: http://localhost:8000/metrics

---

## ğŸ— Visual Architecture Overview

### System Architecture

```mermaid
graph TB
    subgraph "ğŸ¨ User Interface Layer"
        UI[Gradio 5.x Web Interface<br/>ğŸ“± SSR + PWA Support<br/>ğŸ” Authentication]
        API[Health & Metrics API<br/>âš¡ Port 8000<br/>ğŸ“Š Performance Monitoring]
    end
    
    subgraph "ğŸ§  Application Core"
        APP[Main Application<br/>ğŸ¯ app.py<br/>ğŸ”§ Configuration Manager]
        CONFIG[Enhanced Config<br/>ğŸ“ YAML + Environment<br/>ğŸ›¡ï¸ Security Settings]
        AUTH[Security Layer<br/>ğŸ”‘ JWT Authentication<br/>â±ï¸ Rate Limiting]
    end
    
    subgraph "ğŸ” RAG Pipeline"
        EMBED[Multi-Backend Embedder<br/>âš™ï¸ torch/onnx/openvino<br/>ğŸš€ 4x Performance]
        VECTOR[Vector Operations<br/>ğŸ” Query + Upsert<br/>ğŸ“ˆ Dynamic Thresholds]
        CONTEXT[Context Assembly<br/>ğŸ¯ Dynamic Filtering<br/>ğŸ“š Citation Tracking]
    end
    
    subgraph "ğŸ­ MoE Components"
        ROUTER[Expert Router<br/>ğŸ§­ Centroid Management<br/>ğŸ“Š Performance Tracking]
        GATE[Selective Gate<br/>ğŸ¯ Adaptive K-Selection<br/>ğŸ”„ Dynamic Thresholds]
        RERANK[Two-Stage Reranker<br/>ğŸ¯ Cross-Encoder + LLM<br/>ğŸ“ˆ NDCG@10 â‰ˆ 74.30]
    end
    
    subgraph "ğŸ“„ Document Processing"
        PARSE[Multi-Format Parser<br/>ğŸ“‘ PDF/TXT/MD Support<br/>ğŸ” Enhanced Security]
        PROP[LLM Propositionizer<br/>ğŸ§© Atomic Fact Extraction<br/>ğŸ“– Citation Mapping]
        INGEST[Batch Ingestion<br/>ğŸ“Š Progress Tracking<br/>âš¡ Parallel Processing]
    end
    
    subgraph "â˜ï¸ External Services"
        PINECONE[(Pinecone Vector DB<br/>ğŸš€ gRPC + Serverless<br/>ğŸ”„ Auto-scaling)]
        OPENROUTER[OpenRouter API<br/>ğŸ¤– 100+ LLM Models<br/>ğŸ’° Cost Optimization]
        HF[HuggingFace Hub<br/>ğŸ“¦ Model Downloads<br/>ğŸ”„ Version Control]
    end
    
    UI --> APP
    API --> APP
    APP --> CONFIG
    APP --> AUTH
    APP --> EMBED
    EMBED --> VECTOR
    VECTOR --> CONTEXT
    
    VECTOR --> ROUTER
    ROUTER --> GATE  
    GATE --> RERANK
    RERANK --> CONTEXT
    
    APP --> PARSE
    PARSE --> PROP
    PROP --> INGEST
    INGEST --> VECTOR
    
    VECTOR <--> PINECONE
    CONTEXT --> OPENROUTER
    EMBED --> HF
    
    style UI fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    style "MoE Components" fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    style "External Services" fill:#fff3e0,stroke:#e65100,stroke-width:2px
    style "RAG Pipeline" fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
```

### Core Component Relationships

```mermaid
classDiagram
    class Config {
        +str EMBED_MODEL
        +str PINECONE_INDEX
        +int TOP_K
        +bool MOE_ENABLED
        +SecurityConfig security
        +MoEConfig moe_config
        +load_from_yaml()
        +validate_settings()
    }
    
    class EmbeddingService {
        +SentenceTransformer model
        +str backend_type
        +Dict cache
        +encode(texts: List[str])
        +encode_optimized(sentences)
        +warm_up_model()
    }
    
    class VectorStore {
        +Pinecone client
        +str index_name
        +upsert_vectors(vectors)
        +query_similar(query_vector)
        +delete_vectors(ids)
    }
    
    class MoEPipeline {
        +ExpertRouter router
        +SelectiveGate gate
        +TwoStageReranker reranker
        +process_query(query)
        +route_to_experts()
    }
    
    class DocumentProcessor {
        +List parsers
        +LLMPropositionizer propositionizer
        +parse_document(file_path)
        +extract_propositions(paragraphs)
        +create_citations()
    }
    
    class RAGOrchestrator {
        +EmbeddingService embedder
        +VectorStore vector_store
        +MoEPipeline moe_pipeline
        +OpenRouterClient llm_client
        +answer_query(question)
        +assemble_context()
    }
    
    Config --> EmbeddingService
    Config --> VectorStore
    Config --> MoEPipeline
    EmbeddingService --> VectorStore
    VectorStore --> RAGOrchestrator
    MoEPipeline --> RAGOrchestrator
    DocumentProcessor --> VectorStore
    RAGOrchestrator --> Config
```

### Document Ingestion Flow

```mermaid
sequenceDiagram
    participant User
    participant UI as Gradio UI
    participant Parser as Document Parser
    participant Prop as Propositionizer
    participant Embed as Embedding Service
    participant Vector as Vector Store
    participant Monitor as Performance Monitor
    
    User->>UI: Upload Document (PDF/TXT/MD)
    UI->>Parser: parse_document(file_path)
    Parser->>Parser: Extract paragraphs & metadata
    Parser->>Prop: propositionize_paragraphs(paragraphs)
    
    Note over Prop: LLM extracts atomic propositions<br/>with citation spans
    
    Prop-->>Parser: propositions + citations
    Parser->>Embed: encode_batch(propositions)
    
    Note over Embed: Multi-backend encoding<br/>(torch/onnx/openvino)
    
    Embed-->>Parser: embeddings (384-dim vectors)
    Parser->>Vector: upsert_vectors(embeddings + metadata)
    Vector->>Monitor: log_performance_metrics()
    Vector-->>UI: ingestion_complete(stats)
    UI-->>User: âœ… Document processed successfully
```

### Query Processing Pipeline

```mermaid
flowchart TD
    A[User Query] --> B{MoE Enabled?}
    B -->|Yes| C[MoE Pipeline]
    B -->|No| D[Standard RAG]
    
    C --> C1[Expert Router]
    C1 --> C2[Selective Gate]
    C2 --> C3[Retrieve Top-K]
    C3 --> C4[Two-Stage Rerank]
    C4 --> E[Context Assembly]
    
    D --> D1[Embed Query]
    D1 --> D2[Vector Search]
    D2 --> D3[Score Filtering]
    D3 --> E
    
    E --> F[LLM Generation]
    F --> G[Citation Extraction]
    G --> H[Response Assembly]
    H --> I[Cost Tracking]
    I --> J[Final Answer]
    
    style C fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#bbf,stroke:#333,stroke-width:2px
    style J fill:#bfb,stroke:#333,stroke-width:2px
```

### Data Flow Architecture

```mermaid
graph LR
    subgraph "ğŸ“„ Input Layer"
        DOC[Documents<br/>PDF, TXT, MD]
        QUERY[User Queries<br/>Natural Language]
    end
    
    subgraph "ğŸ”„ Processing Layer"
        PARSE[Parse & Extract<br/>Paragraphs + Metadata]
        PROP[Propositionize<br/>Atomic Facts]
        EMBED_DOC[Embed Documents<br/>Dense + Sparse]
        EMBED_Q[Embed Query<br/>Multi-Backend]
    end
    
    subgraph "ğŸ’¾ Storage Layer"
        CACHE[Model Cache<br/>Performance Optimization]
        VECTOR_DB[(Vector Database<br/>Pinecone Serverless)]
        LOGS[(Logs & Metrics<br/>Performance Tracking)]
    end
    
    subgraph "ğŸ§  Intelligence Layer"
        RETRIEVE[Vector Retrieval<br/>Similarity Search]
        MOE[MoE Processing<br/>Expert Routing]
        RERANK[Reranking<br/>Cross-Encoder]
        GENERATE[LLM Generation<br/>OpenRouter API]
    end
    
    subgraph "ğŸ“Š Output Layer"
        ANSWER[Final Answer<br/>With Citations]
        METRICS[Performance Metrics<br/>Cost Analysis]
    end
    
    DOC --> PARSE --> PROP --> EMBED_DOC --> VECTOR_DB
    QUERY --> EMBED_Q --> RETRIEVE
    EMBED_DOC --> CACHE
    EMBED_Q --> CACHE
    
    RETRIEVE --> MOE --> RERANK --> GENERATE
    VECTOR_DB --> RETRIEVE
    GENERATE --> ANSWER
    RERANK --> LOGS
    GENERATE --> METRICS
    
    style "Intelligence Layer" fill:#e1f5fe
    style "Output Layer" fill:#e8f5e8
```

---

## ğŸ“¦ Installation Guide

### Prerequisites

Ensure your system meets these requirements before installation:

| Requirement | Minimum | Recommended | Notes |
|-------------|---------|-------------|--------|
| **Python** | 3.10+ | 3.11+ | Enhanced type hints & performance |
| **RAM** | 8GB | 16GB+ | 20% increase with MoE architecture |
| **Storage** | 10GB | 20GB+ | Model cache & document storage |
| **CPU** | 4 cores | 8+ cores | Multi-backend processing support |
| **GPU** | Optional | RTX 3060+ | 4x performance with OpenVINO |

### Required API Keys

You'll need accounts and API keys for:

- **[OpenRouter](https://openrouter.ai/)**: Access to 100+ language models ($5 minimum credit)
- **[Pinecone](https://www.pinecone.io/)**: Serverless vector database (free tier available)

### Step-by-Step Installation

#### 1. Clone the Repository

```bash
git clone https://github.com/JackSmack1971/personal-rag-propo.git
cd personal-rag-propo
```

#### 2. Create Virtual Environment

<details>
<summary>ğŸªŸ Windows (PowerShell)</summary>

```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1

# If you encounter execution policy issues
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

</details>

<details>
<summary>ğŸ macOS/Linux</summary>

```bash
python3 -m venv venv
source venv/bin/activate

# Verify activation
which python  # Should point to venv/bin/python
```

</details>

#### 3. Install Dependencies

```bash
# Upgrade pip for enhanced dependency resolution
pip install --upgrade pip

# Install 2025 enhanced dependencies
pip install -r requirements-2025.txt

# Optional: Install development dependencies  
pip install -r requirements-dev.txt
```

#### 4. Configure Environment

```bash
# Copy environment template
cp .env.example .env

# Edit configuration file
nano .env  # or code .env for VS Code
```

**Required Environment Variables:**

```env
# ============= API Keys (Required) =============
OPENROUTER_API_KEY=your_openrouter_api_key_here
PINECONE_API_KEY=your_pinecone_api_key_here

# ============= Core Configuration =============
PINECONE_INDEX=personal-rag
EMBED_MODEL=BAAI/bge-small-en-v1.5
NAMESPACE=default
TOP_K=6

# ============= 2025 Stack Features =============
GRADIO_SSR_ENABLED=true
PINECONE_GRPC_ENABLED=true
SENTENCE_TRANSFORMERS_BACKEND=torch

# ============= Security Settings =============
GRADIO_AUTH_ENABLED=false
GRADIO_AUTH_USER=admin
GRADIO_AUTH_PASS=secure_password_here

# ============= Performance Tuning =============
ENABLE_MODEL_CACHING=true
CACHE_TTL_SECONDS=3600
MAX_BATCH_SIZE=32

# ============= Advanced Features =============
MOE_ENABLED=false  # Set to true for enhanced retrieval
COST_MONITORING_ENABLED=true
PERFORMANCE_LOGGING=true
```

#### 5. Initialize Pinecone Index

```bash
python -c "
from src.config import Config
from src.vectorstore import ensure_index_exists

try:
    cfg = Config()
    ensure_index_exists(cfg, dim=384)
    print('âœ… Pinecone index initialized successfully')
    print(f'   Index: {cfg.PINECONE_INDEX}')
    print(f'   Dimension: 384')
except Exception as e:
    print(f'âŒ Initialization failed: {e}')
"
```

#### 6. Verify Installation

```bash
# Run comprehensive system health check
python -c "
import sys
print(f'âœ… Python {sys.version}')

# Test core imports
try:
    from src.config import Config
    from src.embeddings import get_embedder  
    from src.vectorstore import VectorStore
    print('âœ… All core modules imported successfully')
except ImportError as e:
    print(f'âŒ Import failed: {e}')
    exit(1)

# Test configuration
try:
    cfg = Config()
    print(f'âœ… Configuration loaded: {cfg.EMBED_MODEL}')
    print(f'   MoE Enabled: {cfg.MOE_ENABLED}')
    print(f'   Security: {\"Enabled\" if cfg.GRADIO_AUTH_ENABLED else \"Disabled\"}')
except Exception as e:
    print(f'âŒ Configuration failed: {e}')
    exit(1)

# Test embedder (downloads model on first run)
try:
    embedder = get_embedder(cfg.EMBED_MODEL)
    test_embedding = embedder.encode('Hello, world!')
    print(f'âœ… Embedder functional: {test_embedding.shape}')
    print('âœ… Installation verification complete!')
except Exception as e:
    print(f'âŒ Embedder test failed: {e}')
"
```

Expected output:
```
âœ… Python 3.11.x
âœ… All core modules imported successfully
âœ… Configuration loaded: BAAI/bge-small-en-v1.5
   MoE Enabled: false
   Security: Disabled
âœ… Embedder functional: (384,)
âœ… Installation verification complete!
```

#### 7. Launch Application

```bash
# Start the enhanced Gradio interface
python app.py
```

**Access Points:**
- **ğŸ¨ Main Application**: http://localhost:7860
- **â¤ï¸ Health Check**: http://localhost:8000/health  
- **ğŸ“Š Performance Metrics**: http://localhost:8000/metrics

### ğŸš¨ Troubleshooting Installation Issues

<details>
<summary>ğŸ”§ Common Issues and Solutions</summary>

**Dependencies Installation Failed**
```bash
# Clear pip cache and reinstall
pip cache purge
pip install --no-cache-dir -r requirements-2025.txt

# If specific package fails
pip install --no-deps package_name
```

**CUDA/GPU Issues**
```bash
# Install CPU-only PyTorch if GPU issues persist
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```

**Port Already in Use**
```bash
# Change default ports in .env
GRADIO_SERVER_PORT=7861
HEALTH_CHECK_PORT=8001

# Or find and kill existing process
lsof -ti:7860 | xargs kill -9  # macOS/Linux
netstat -ano | findstr :7860  # Windows
```

**API Key Issues**  
- Verify API keys are active with sufficient credits
- Check for extra spaces or hidden characters in `.env`
- Test API keys independently:
```bash
curl -H "Authorization: Bearer YOUR_OPENROUTER_KEY" https://openrouter.ai/api/v1/models
```

**Memory Issues**
```bash
# Reduce batch size in .env
MAX_BATCH_SIZE=16

# Enable model caching optimization
ENABLE_MODEL_CACHING=true
```

</details>

---

## ğŸš€ Usage Instructions

### Basic Document Processing

#### Upload Documents via Web Interface

1. **Start the Application**
   ```bash
   python app.py
   ```

2. **Access the Interface** 
   - Navigate to http://localhost:7860
   - The modern Gradio 5.x interface will load with SSR optimization

3. **Upload Documents**
   - Click the "ğŸ“„ Document Upload" tab
   - Drag & drop or select files (PDF, TXT, MD)
   - Maximum: 10MB per file, supports batch uploads

4. **Monitor Processing**
   - Real-time progress bars show ingestion status
   - Proposition extraction and embedding progress
   - Final statistics display (documents processed, propositions extracted)

5. **Start Querying**
   - Switch to "ğŸ’¬ Chat" tab
   - Ask questions about your uploaded documents
   - Receive answers with precise citations and source references

#### Command Line Document Ingestion

For batch processing or automation:

```bash
# Ingest single document
python -c "
from src.ingest import ingest_files
from src.config import Config
from src.embeddings import get_embedder
from pathlib import Path

cfg = Config()
embedder = get_embedder(cfg.EMBED_MODEL)
files = [Path('your_document.pdf')]
report = ingest_files(cfg, embedder, files, 'default')
print(f'Processed: {report}')
"

# Batch ingest directory
python scripts/batch_ingest.py --directory ./documents --namespace research_papers
```

### Advanced Query Techniques

#### Using MoE Enhanced Retrieval

Enable Mixture of Experts for improved accuracy:

```bash
# Enable in .env file
MOE_ENABLED=true
MOE_NUM_EXPERTS=4
MOE_RERANK_TOP_K=20
```

**Query Examples:**
- **Factual Questions**: "What are the key findings of the 2023 climate report?"
- **Comparative Analysis**: "Compare the methodologies used in papers A and B"  
- **Citation Hunting**: "Find all references to machine learning in my documents"
- **Summarization**: "Summarize the main arguments across all uploaded papers"

#### Cost Estimation & Monitoring

1. Navigate to "ğŸ’° Cost Estimation" tab
2. Input expected monthly usage:
   - **Monthly Queries**: Number of questions you plan to ask
   - **Average Prompt Tokens**: Typical context size (default: 2000)
   - **Average Completion Tokens**: Expected response length (default: 500)
3. Get real-time cost projections with breakdown by model

#### Performance Optimization

**Caching Configuration**
```python
# Configure in Python or via environment
from src.config import Config

config = Config()
config.ENABLE_MODEL_CACHING = True
config.CACHE_TTL_SECONDS = 3600  # 1 hour cache

# Clear caches when needed
from src.cache import clear_all_caches
clear_all_caches()
```

**Batch Processing for Large Collections**
```python
from src.batch_processing import BatchProcessor

processor = BatchProcessor(config)
results = processor.process_directory(
    directory_path="documents/", 
    batch_size=10,
    parallel_workers=4
)
```

### API Usage (Advanced)

For integration with other applications:

```python
from src.rag import RAGOrchestrator
from src.config import Config

# Initialize RAG system
config = Config()
rag = RAGOrchestrator(config)

# Process query
result = rag.answer_query(
    question="What are the benefits of renewable energy?",
    namespace="research_papers",
    include_citations=True
)

print(f"Answer: {result.answer}")
print(f"Citations: {result.citations}")
print(f"Cost: ${result.cost_info.total_cost:.4f}")
```

---

## ğŸ“ Project Structure

```
personal-rag-propo/
â”œâ”€â”€ ğŸ“„ app.py                          # ğŸ¯ Main Gradio application entry point
â”œâ”€â”€ ğŸ“„ requirements-2025.txt           # ğŸ“¦ Enhanced dependency specifications
â”œâ”€â”€ ğŸ“„ .env.example                    # âš™ï¸ Environment configuration template
â”œâ”€â”€ ğŸ“„ README.md                       # ğŸ“š This comprehensive documentation
â”œâ”€â”€ ğŸ“„ LICENSE                         # âš–ï¸ MIT License
â”œâ”€â”€ ğŸ“„ CONTRIBUTING.md                 # ğŸ¤ Contribution guidelines
â”‚
â”œâ”€â”€ ğŸ“‚ src/                            # ğŸ— Core application modules
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py                 # Package initialization
â”‚   â”œâ”€â”€ ğŸ“„ config.py                   # âš™ï¸ Enhanced configuration with YAML support
â”‚   â”œâ”€â”€ ğŸ“„ embeddings.py               # ğŸ§  Multi-backend embeddings (torch/onnx/openvino)
â”‚   â”œâ”€â”€ ğŸ“„ vectorstore.py              # ğŸ—„ï¸ Pinecone 7.x with gRPC integration  
â”‚   â”œâ”€â”€ ğŸ“„ rag.py                      # ğŸ” RAG pipeline with MoE integration
â”‚   â”œâ”€â”€ ğŸ“„ ingest.py                   # ğŸ“¥ Document ingestion orchestration
â”‚   â”œâ”€â”€ ğŸ“„ parsers.py                  # ğŸ“‘ Multi-format document parsers
â”‚   â”œâ”€â”€ ğŸ“„ propositionizer.py          # ğŸ§© LLM-based atomic fact extraction
â”‚   â”œâ”€â”€ ğŸ“„ monitoring.py               # ğŸ“Š Cost tracking and performance metrics
â”‚   â”œâ”€â”€ ğŸ“„ security.py                 # ğŸ”’ Security validation and hardening
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ moe/                        # ğŸ­ Mixture of Experts implementation  
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py              # MoE package initialization
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ config.py                # MoE-specific configuration
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ router.py                # ğŸ§­ Expert routing with centroid management
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ gate.py                  # ğŸšª Selective retrieval gating
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ rerank.py                # ğŸ¯ Two-stage reranking pipeline
â”‚   â”‚   â””â”€â”€ ğŸ“„ evaluation.py            # ğŸ“ˆ MoE performance monitoring
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ eval/                       # ğŸ§ª Comprehensive evaluation framework
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py              # Evaluation package initialization
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ metrics.py               # ğŸ“Š Hit@k, nDCG@k, span accuracy
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ benchmark.py             # ğŸ Automated benchmarking suite
â”‚   â”‚   â””â”€â”€ ğŸ“„ ab_testing.py            # âš–ï¸ A/B testing capabilities
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ auth/                       # ğŸ” Authentication and authorization
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py              # Auth package initialization
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ jwt_handler.py           # ğŸ« JWT token management
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ rate_limiter.py          # â±ï¸ Rate limiting implementation
â”‚   â”‚   â””â”€â”€ ğŸ“„ session_manager.py       # ğŸ‘¥ User session management
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ utils/                      # ğŸ›  Utility functions and helpers
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py              # Utils package initialization
â”‚       â”œâ”€â”€ ğŸ“„ logging.py               # ğŸ“ Enhanced logging configuration
â”‚       â”œâ”€â”€ ğŸ“„ cache.py                 # ğŸ’¾ Intelligent caching system
â”‚       â””â”€â”€ ğŸ“„ helpers.py               # ğŸ”§ Common utility functions
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                          # ğŸ§ª Comprehensive testing suite
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py                 # Test package initialization
â”‚   â”œâ”€â”€ ğŸ“„ conftest.py                 # ğŸ”§ Pytest configuration and fixtures
â”‚   â”œâ”€â”€ ğŸ“„ test_config.py              # âš™ï¸ Configuration testing
â”‚   â”œâ”€â”€ ğŸ“„ test_embeddings.py          # ğŸ§  Embedding system tests
â”‚   â”œâ”€â”€ ğŸ“„ test_vectorstore.py         # ğŸ—„ï¸ Vector storage tests
â”‚   â”œâ”€â”€ ğŸ“„ test_rag.py                 # ğŸ” RAG pipeline tests
â”‚   â”œâ”€â”€ ğŸ“„ test_moe.py                 # ğŸ­ MoE system tests
â”‚   â”œâ”€â”€ ğŸ“„ test_security.py            # ğŸ”’ Security validation tests
â”‚   â”œâ”€â”€ ğŸ“„ test_integration.py         # ğŸ”— End-to-end integration tests
â”‚   â””â”€â”€ ğŸ“„ test_performance.py         # âš¡ Performance benchmark tests
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                        # ğŸ”§ Automation and utility scripts
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py                 # Scripts package initialization
â”‚   â”œâ”€â”€ ğŸ“„ batch_ingest.py             # ğŸ“¥ Batch document processing
â”‚   â”œâ”€â”€ ğŸ“„ benchmark.py                # ğŸ“Š Performance benchmarking
â”‚   â”œâ”€â”€ ğŸ“„ migration.py                # ğŸ”„ Database migration utilities
â”‚   â”œâ”€â”€ ğŸ“„ cleanup.py                  # ğŸ§¹ Cache and data cleanup
â”‚   â””â”€â”€ ğŸ“„ deploy.py                   # ğŸš€ Production deployment automation
â”‚
â”œâ”€â”€ ğŸ“‚ data/                           # ğŸ’¾ Data storage directory
â”‚   â”œâ”€â”€ ğŸ“‚ uploads/                    # ğŸ“ Uploaded document storage
â”‚   â”œâ”€â”€ ğŸ“‚ cache/                      # ğŸ—ƒï¸ Model and result caching
â”‚   â””â”€â”€ ğŸ“‚ exports/                    # ğŸ“¤ Data export directory
â”‚
â”œâ”€â”€ ğŸ“‚ logs/                           # ğŸ“‹ Application logging
â”‚   â”œâ”€â”€ ğŸ“„ app.log                     # General application logs
â”‚   â”œâ”€â”€ ğŸ“„ security.log               # ğŸ”’ Security event logging
â”‚   â””â”€â”€ ğŸ“„ performance.log             # âš¡ Performance monitoring logs
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                           # ğŸ“– Extended documentation
â”‚   â”œâ”€â”€ ğŸ“„ ARCHITECTURE.md             # ğŸ— Detailed architecture guide
â”‚   â”œâ”€â”€ ğŸ“„ API_REFERENCE.md            # ğŸ“š Complete API documentation
â”‚   â”œâ”€â”€ ğŸ“„ DEPLOYMENT.md               # ğŸš€ Production deployment guide
â”‚   â”œâ”€â”€ ğŸ“„ SECURITY.md                 # ğŸ”’ Security implementation guide
â”‚   â””â”€â”€ ğŸ“„ TROUBLESHOOTING.md          # ğŸ”§ Common issues and solutions
â”‚
â”œâ”€â”€ ğŸ“‚ configs/                        # âš™ï¸ Configuration files
â”‚   â”œâ”€â”€ ğŸ“„ default.yaml                # Default configuration settings
â”‚   â”œâ”€â”€ ğŸ“„ production.yaml             # Production environment config
â”‚   â””â”€â”€ ğŸ“„ development.yaml            # Development environment config
â”‚
â””â”€â”€ ğŸ“‚ memory-bank/                    # ğŸ§  Project context and specifications
    â”œâ”€â”€ ğŸ“„ productContext.md           # ğŸ“‹ Product requirements document
    â”œâ”€â”€ ğŸ“„ qa_test_plan.md             # ğŸ§ª Comprehensive QA testing plan
    â””â”€â”€ ğŸ“„ AGENTS.md                   # ğŸ¤– AI collaboration guide
```

### Key Directories Explained

<details>
<summary><strong>ğŸ“‚ src/ - Core Application Modules</strong></summary>

**Primary Modules:**
- `config.py`: Enhanced configuration management with YAML support, environment validation, and MoE settings
- `embeddings.py`: Multi-backend sentence transformers with performance optimizations and intelligent caching
- `vectorstore.py`: Pinecone 7.x integration with gRPC performance improvements and enhanced error handling
- `rag.py`: Complete RAG pipeline orchestration with optional MoE enhancement and real-time cost monitoring

**Specialized Directories:**
- `moe/`: Complete Mixture of Experts implementation with routing, gating, and reranking components
- `eval/`: Comprehensive evaluation framework with retrieval metrics, automated benchmarking, and A/B testing
- `auth/`: Enterprise-grade authentication with JWT tokens, rate limiting, and session management
- `utils/`: Common utilities including intelligent caching, enhanced logging, and helper functions

</details>

<details>
<summary><strong>ğŸ“‚ tests/ - Comprehensive Testing Suite</strong></summary>

**Test Coverage Areas:**
- **Unit Tests**: Individual component testing with >90% coverage
- **Integration Tests**: End-to-end pipeline validation  
- **Performance Tests**: Benchmarking and load testing
- **Security Tests**: Authentication, authorization, and input validation
- **MoE Tests**: Specialized testing for Mixture of Experts components

</details>

<details>
<summary><strong>ğŸ“‚ scripts/ - Automation Utilities</strong></summary>

**Available Scripts:**
- `batch_ingest.py`: Efficient batch processing of large document collections
- `benchmark.py`: Performance benchmarking with detailed metrics
- `migration.py`: Database schema migration and data transformation utilities
- `deploy.py`: Production deployment automation with health checks

</details>

---

## ğŸ›  Development Guide

### Development Environment Setup

#### 1. Clone and Setup Development Environment

```bash
# Clone repository
git clone https://github.com/JackSmack1971/personal-rag-propo.git
cd personal-rag-propo

# Create development virtual environment
python3 -m venv venv-dev
source venv-dev/bin/activate  # or .\venv-dev\Scripts\activate on Windows

# Install development dependencies
pip install -r requirements-dev.txt
```

#### 2. Pre-commit Hooks Setup

```bash
# Install pre-commit hooks for code quality
pre-commit install

# Run hooks manually
pre-commit run --all-files
```

#### 3. Development Configuration

Create `.env.development`:
```env
# Development-specific settings
DEBUG=true
LOG_LEVEL=DEBUG
GRADIO_DEBUG=true

# Use smaller models for faster development
EMBED_MODEL=BAAI/bge-small-en-v1.5
MOE_ENABLED=false

# Development API endpoints
PINECONE_ENVIRONMENT=development
OPENROUTER_DEBUG=true
```

### Build and Test Procedures

#### Running Tests

```bash
# Run all tests with coverage
pytest tests/ --cov=src --cov-report=html --cov-report=term-missing

# Run specific test categories
pytest tests/test_embeddings.py -v          # Embedding tests
pytest tests/test_moe.py -v                 # MoE system tests  
pytest tests/test_integration.py -v         # Integration tests
pytest tests/test_performance.py -v         # Performance benchmarks

# Run tests with specific markers
pytest -m "not slow" -v                     # Skip slow tests
pytest -m "security" -v                     # Run security tests only
```

#### Performance Benchmarking

```bash
# Run comprehensive performance benchmarks
python scripts/benchmark.py --full-suite

# Benchmark specific components
python scripts/benchmark.py --component embeddings
python scripts/benchmark.py --component moe
python scripts/benchmark.py --component retrieval
```

#### Code Quality Checks

```bash
# Format code with black
black src/ tests/ scripts/

# Sort imports with isort  
isort src/ tests/ scripts/

# Type checking with mypy
mypy src/

# Linting with flake8
flake8 src/ tests/ scripts/

# Security scanning with bandit
bandit -r src/
```

### Contribution Guidelines

#### Code Style Conventions

- **Python Style**: Follow PEP 8 with line length of 88 characters (Black default)
- **Type Hints**: Mandatory for all function signatures and class attributes
- **Docstrings**: Google-style docstrings for all public functions and classes
- **Import Organization**: Use isort with the configuration in `pyproject.toml`

#### Git Workflow

```bash
# 1. Create feature branch
git checkout -b feature/your-feature-name

# 2. Make changes and commit
git add .
git commit -m "feat: add new feature description"

# 3. Push and create PR
git push origin feature/your-feature-name
```

#### Commit Message Convention

Follow [Conventional Commits](https://conventionalcommits.org/):
- `feat:` New feature
- `fix:` Bug fix  
- `docs:` Documentation changes
- `style:` Code style changes (formatting, etc.)
- `refactor:` Code refactoring
- `test:` Adding/updating tests
- `perf:` Performance improvements

#### Pull Request Process

1. **Pre-PR Checklist**:
   - [ ] All tests pass locally
   - [ ] Code coverage maintains >90%
   - [ ] Documentation updated
   - [ ] Security review completed

2. **PR Description Template**:
   ```markdown
   ## Description
   Brief description of changes
   
   ## Type of Change
   - [ ] Bug fix
   - [ ] New feature  
   - [ ] Breaking change
   - [ ] Documentation update
   
   ## Testing
   - [ ] Unit tests added/updated
   - [ ] Integration tests pass
   - [ ] Manual testing completed
   
   ## Security Considerations
   - [ ] No sensitive data exposed
   - [ ] Input validation implemented
   - [ ] Authentication/authorization reviewed
   ```

### Architecture Decision Records (ADRs)

Document major architectural decisions in `docs/adr/`:

```markdown
# ADR-001: Adoption of Mixture of Experts Architecture

## Status
Accepted

## Context  
Need to improve retrieval accuracy for complex queries...

## Decision
Implement MoE architecture with expert routing...

## Consequences
- Improved accuracy by 15-20%
- Increased complexity in codebase
- Higher computational requirements
```

### Local Development Tips

#### Quick Development Commands

```bash
# Start development server with auto-reload
python app.py --reload

# Run with specific configuration
python app.py --config configs/development.yaml

# Enable debug logging
LOG_LEVEL=DEBUG python app.py

# Test specific features
python -c "from src.moe import test_moe_pipeline; test_moe_pipeline()"
```

#### Debugging Tools

```python
# Enable debug mode in embeddings
from src.embeddings import get_embedder
embedder = get_embedder("BAAI/bge-small-en-v1.5", debug=True)

# Monitor performance metrics
from src.monitoring import get_performance_stats
stats = get_performance_stats()
print(stats)

# Test vector operations
from src.vectorstore import VectorStore
vs = VectorStore(debug=True)
vs.test_connection()
```

---

## ğŸ“š Additional Documentation

### Extended Resources

- **[ğŸ“– Complete API Reference](docs/API_REFERENCE.md)**: Detailed API documentation with examples
- **[ğŸ— Architecture Guide](docs/ARCHITECTURE.md)**: Deep dive into system architecture and design patterns  
- **[ğŸš€ Deployment Guide](docs/DEPLOYMENT.md)**: Production deployment strategies and best practices
- **[ğŸ”’ Security Guide](docs/SECURITY.md)**: Security implementation details and compliance
- **[ğŸ”§ Troubleshooting Guide](docs/TROUBLESHOOTING.md)**: Common issues and comprehensive solutions

### Frequently Asked Questions

<details>
<summary><strong>â“ How do I enable MoE (Mixture of Experts) mode?</strong></summary>

Set `MOE_ENABLED=true` in your `.env` file. This enables:
- Expert routing based on query similarity
- Selective retrieval gating with adaptive K-selection  
- Two-stage reranking with cross-encoder models
- Performance monitoring and A/B testing capabilities

Note: MoE mode requires additional computational resources but provides 15-20% improvement in retrieval accuracy.

</details>

<details>
<summary><strong>ğŸ’° How much does it cost to run this system?</strong></summary>

Costs depend on usage patterns:
- **Embedding Models**: Free (runs locally)
- **Pinecone**: $0.096/hour for 1 pod (free tier available)
- **OpenRouter**: ~$0.002-$0.02 per query depending on model chosen
- **Estimated Monthly**: $10-50 for moderate usage (100-500 queries/month)

Use the built-in cost estimation tool for personalized projections.

</details>

<details>
<summary><strong>ğŸ”’ Is my data secure and private?</strong></summary>

Yes, the system prioritizes privacy:
- Documents processed locally, never sent to third parties for embedding
- Only user queries and retrieved context sent to LLM providers
- Optional authentication and rate limiting
- Comprehensive security logging
- SSL/HTTPS support for production deployments

See the [Security Guide](docs/SECURITY.md) for detailed security measures.

</details>

<details>
<summary><strong>âš¡ How can I improve performance?</strong></summary>

Several optimization strategies:
- Enable model caching (`ENABLE_MODEL_CACHING=true`)
- Use OpenVINO backend for 4x CPU performance (`SENTENCE_TRANSFORMERS_BACKEND=openvino`)
- Optimize batch sizes (`MAX_BATCH_SIZE=32`)
- Enable MoE mode for better retrieval accuracy
- Use GPU acceleration if available

</details>

<details>
<summary><strong>ğŸ“„ What document formats are supported?</strong></summary>

Currently supported formats:
- **PDF**: Full text extraction with metadata preservation
- **TXT**: Plain text with encoding auto-detection
- **MD**: Markdown with structure preservation

Planned support: DOCX, EPUB, HTML, CSV, JSON

</details>

### Changelog and Version History

#### Version 2025.1.0 (Current)
- ğŸ‰ **Major Technology Stack Upgrade**: Gradio 5.x, PyTorch 2.8.x, Pinecone 7.x
- ğŸ­ **MoE Architecture**: Complete Mixture of Experts implementation
- âš¡ **Performance**: 4x inference speedup with OpenVINO, enhanced caching
- ğŸ”’ **Security**: JWT authentication, rate limiting, SSL/HTTPS support
- ğŸ“Š **Monitoring**: Real-time cost tracking, performance metrics, health checks
- ğŸ§ª **Testing**: Comprehensive test suite with >90% coverage

#### Previous Versions
- **2024.3.0**: Initial MoE research integration
- **2024.2.0**: Enhanced security and authentication  
- **2024.1.0**: Multi-backend embedding support
- **2023.4.0**: Original release with basic RAG pipeline

### License and Legal

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

**Third-Party Licenses:**
- Sentence-Transformers: Apache 2.0
- Gradio: Apache 2.0  
- PyTorch: BSD 3-Clause
- Pinecone Python Client: Apache 2.0

### Credits and Acknowledgments

#### Core Contributors
- **JackSmack1971**: Original creator and maintainer
- **Community Contributors**: Feature enhancements and bug fixes

#### Technology Partners
- **[Sentence Transformers](https://sbert.net/)**: Excellent embedding framework and model ecosystem
- **[Pinecone](https://pinecone.io/)**: Robust serverless vector database infrastructure  
- **[Gradio](https://gradio.app/)**: Intuitive web interface framework with production capabilities
- **[OpenRouter](https://openrouter.ai/)**: Democratized access to 100+ language models
- **[HuggingFace](https://huggingface.co/)**: Model hosting and transformer ecosystem

#### Research Citations

This system builds upon cutting-edge research in retrieval-augmented generation:
- **Dense Passage Retrieval**: Karpukhin et al. (2020)
- **Mixture of Experts**: Shazeer et al. (2017)  
- **Sentence Embeddings**: Reimers & Gurevych (2019)
- **Cross-Encoder Reranking**: Nogueira & Cho (2019)

---

<div align="center">

### ğŸŒŸ **Star this repository if you find it useful!** ğŸŒŸ

[![GitHub Stars](https://img.shields.io/github/stars/JackSmack1971/personal-rag-propo?style=social)](https://github.com/JackSmack1971/personal-rag-propo/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/JackSmack1971/personal-rag-propo?style=social)](https://github.com/JackSmack1971/personal-rag-propo/network)

**[ğŸ“ Report Bug](https://github.com/JackSmack1971/personal-rag-propo/issues)** â€¢ 
**[ğŸ’¡ Request Feature](https://github.com/JackSmack1971/personal-rag-propo/issues)** â€¢ 
**[ğŸ“– Documentation](docs/)** â€¢ 
**[ğŸ’¬ Discussions](https://github.com/JackSmack1971/personal-rag-propo/discussions)**

</div>

---

*ğŸ“… Generated on: Sunday, August 31, 2025*  
*ğŸ” Analysis Timestamp: 2025-08-31T16:50:00Z*  
*ğŸ“¦ Repository Version: Enhanced 2025 Stack with Advanced MoE Retrieval*  
*ğŸ¤– Documentation Agent: Claude Sonnet 4*
